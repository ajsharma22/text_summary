{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "stopwords=list(STOP_WORDS)\n",
    "from string import punctuation\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp_spacy = spacy.load('en_core_web_sm')\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class text_summary:\n",
    "    def __init__(self, model='nltk'):\n",
    "        self.model = model\n",
    "#         from string import punctuation\n",
    "#         if self.model =='nltk':\n",
    "#             import nltk\n",
    "#             nltk.download('punkt')\n",
    "#         elif self.model =='spacy':\n",
    "#             import spacy\n",
    "#             from spacy.lang.en.stop_words import STOP_WORDS\n",
    "#             stopwords=list(STOP_WORDS)\n",
    "#             nlp_spacy = spacy.load('en_core_web_sm')\n",
    "#         else:\n",
    "#             pass\n",
    "\n",
    "    \n",
    "    def clean_words(self, text, remove_stop_words = True):\n",
    "        \"\"\"it removes the punctuation and numbers from the text, optional: stop words\"\"\"\n",
    "        if remove_stop_words:\n",
    "            text = text.split()\n",
    "#             print(text)\n",
    "            text = \" \".join([x for x in text if x not in stopwords])\n",
    "        text  = \"\".join([char for char in text if char not in punctuation])\n",
    "        text = re.sub('[0-9]+', '', text)   \n",
    "        text = text.lower().replace('\\n', ' ')\n",
    "        text = re.sub(\"\\s+\", \" \", text)\n",
    "#         print(text)\n",
    "        return text\n",
    "    \n",
    "    def clean_data_text(self, df):\n",
    "        self.data = df\n",
    "    # also included removing stop words part but not in this case later we will check\n",
    "        self.data['tokenized_sents'] = self.data['tokenized_sents'].apply(self.clean_words)\n",
    "    \n",
    "        return self.data\n",
    "    \n",
    "\n",
    "    def create_sent(self, df):\n",
    "\n",
    "    #     self.data['sentences_list'] = self.data['article_text'].apply(nltk.sent_tokenize)\n",
    "        self.data = df\n",
    "        self.data['article_text'] =self.data['article_text'].astype('str')\n",
    "        if self.model =='spacy':\n",
    "            self.data['nlp_docs'] = self.data['article_text'].apply(nlp_spacy)\n",
    "            # iterating over all the every doc to get the sentecnces\n",
    "            self.data['sentence_list'] = ''\n",
    "            for i in range(self.data.shape[0]):\n",
    "                sentence=  [str(sent) for sent in self.data['nlp_docs'][i].sents]\n",
    "                self.data.at[self.data.index[i],'sentence_list'] = sentence\n",
    "\n",
    "        elif self.model == 'nltk':\n",
    "            self.data['sentence_list'] = self.data['article_text'].apply(nltk.sent_tokenize)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        self.data['tokenized_sents'] = self.data['article_text'].astype(str)\n",
    "        self.data = self.clean_data_text(self.data) # cleaning the article\n",
    "        self.data['tokenized_sents_copy'] = self.data['tokenized_sents'] #creating copy of sentence without changes\n",
    "        return self.data\n",
    "    \n",
    "    def create_words_frequency_dictionary(self, df):\n",
    "        self.data = df\n",
    "        from collections import Counter\n",
    "        self.data['frequency'] = self.data['tokenized_sents_copy'].str.split(' ').apply(Counter)\n",
    "        return self.data\n",
    "    \n",
    "    def get_maximum_frequency(self, freq_dic):\n",
    "        return max(freq_dic.values())\n",
    "    \n",
    "    def create_word_weighted_freq(self, row_dictionary):\n",
    "        max_weight = self.get_maximum_frequency(row_dictionary)\n",
    "        for key, val in row_dictionary.items():\n",
    "            row_dictionary[key] = val/max_weight\n",
    "        return row_dictionary\n",
    "    \n",
    "#     return max key value pair\n",
    "#     this is not needed but still created it for future use\n",
    "    def get_max_key_value(self, freq_dic):\n",
    "        max_key_value_dic = {}\n",
    "        k = max(freq_dic, key = freq_dic.get)\n",
    "        max_key_value_dic[k] = freq_dic[k]\n",
    "        return max_key_value_dic\n",
    "    \n",
    "#     test_function\n",
    "    def insert_max_word_key_value(self, df):\n",
    "        self.data = df\n",
    "        self.data['max'] = self.data['frequency'].apply(self.get_max_key_value)\n",
    "        return self.data\n",
    "    \n",
    "    def calculate_sentence_score(self, sentence_list, \n",
    "                             word_weight_dictionary,\n",
    "                            decided_len=30, short_sent_length = 5):\n",
    "        \"\"\"This function calculate the sentence score of a sentence by \n",
    "        summing up the weighted score of the words. \"\"\"\n",
    "        sentence_score = {}\n",
    "    #     print('step 1')\n",
    "        for sent in sentence_list:\n",
    "            sent_len = len(sent.split(' '))\n",
    "            if ((sent_len < decided_len) and (sent_len > short_sent_length)):\n",
    "                for word in sent.lower():\n",
    "                    if sent not in sentence_score.keys():\n",
    "                        sentence_score[sent] = word_weight_dictionary[word]\n",
    "                    else:\n",
    "                        sentence_score[sent] += word_weight_dictionary[word]\n",
    "  \n",
    "        return [sentence_score]\n",
    "\n",
    "    def remove_ascii(self, text):\n",
    "        text = str(text)\n",
    "        text = ''.join((c for c in text if ord(c) < 128))\n",
    "        return text\n",
    "            \n",
    "    \n",
    "    \n",
    "    def topic_tagging(self, topic_dict, text):\n",
    "        \"\"\"This function tag the articles with topics using\n",
    "        the given keywords dictionary.\n",
    "        \"\"\"\n",
    "        tagg_value_dic = {}\n",
    "        # initialising a dictionary with keys only\n",
    "        for k in topic_dict.keys():\n",
    "            tagg_value_dic[k] = 0\n",
    "        clean_text = self.clean_words(text)\n",
    "        clean_text = clean_text.split()\n",
    "\n",
    "        for k, v in topic_dict.items():\n",
    "            for word in clean_text:\n",
    "                if word in v:\n",
    "                    tagg_value_dic[k] += 1\n",
    "                \n",
    "        return tagg_value_dic\n",
    "    \n",
    "    \n",
    "    def create_synonms_from_keywords(self, kwords):\n",
    "        \"\"\"This function creates a dictionary of synonyms \n",
    "        of words which can be used in getting the summary.\"\"\"\n",
    "        synonyms_dict={}\n",
    "        \n",
    "        keywords = kwords\n",
    "        \n",
    "        for word1 in keywords:\n",
    "            synonyms_dict[word1]={}\n",
    "            synonyms=[]\n",
    "            for syn in wordnet.synsets(word1):\n",
    "                #print(\"Yes\")\n",
    "                for l in syn.lemmas():\n",
    "                    synonyms.append(l.name())\n",
    "            synonyms_dict[word1]=(synonyms)\n",
    "        print(synonyms_dict)\n",
    "        return synonyms_dict\n",
    "    \n",
    "    def insert_topics_of_article(self, data, topic_dict):\n",
    "        self.df = data\n",
    "        for i in range(self.df.shape[0]):\n",
    "            self.df.at[i, 'topics'] = self.topic_tagging(topic_dict,\n",
    "                                                         self.df.at[i,'article_text'])\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    def create_summary(self, data, \n",
    "                       keywords = [],\n",
    "                       summary_sent_len =7):\n",
    "        \"\"\"It calculates the summary on the basis of the highest sentence score.\n",
    "        It also has a optional keywords parameter which can be used to pass a list \n",
    "        of words, if present in any sentence then it will be part of the summary, no matter\n",
    "        what is the sentence summary.\"\"\"\n",
    "        self.df = data\n",
    "        self.df['sentences_scores_dict'] = 0\n",
    "        self.df['summary'] = ''\n",
    "        \n",
    "        self.df['summary'] = self.df['summary'].astype('O')\n",
    "        for i in range(self.df.shape[0]):\n",
    "            dictionary_of_sent_scores = self.calculate_sentence_score(self.df['sentence_list'][i],\n",
    "                                                                 word_weight_dictionary=self.df['weighted_freq'][i]) \n",
    "\n",
    "            \n",
    "\n",
    "            dictionary_of_sent_scores = dictionary_of_sent_scores[0]\n",
    "            sorted_dict = sorted(dictionary_of_sent_scores,\n",
    "                                         key = dictionary_of_sent_scores.get,\n",
    "                                reverse=True)\n",
    "            \n",
    "#             if keywords are passed then adding them by increasing the score\n",
    "#             of those sentences which contains these keywords and also increasing\n",
    "#             the variable length so that it doesn't skip important sentences\n",
    "#             on the basis of weights.\n",
    "            keywords_sentences_list = [] \n",
    "            if len(keywords) > 0:\n",
    "                \n",
    "                for k in dictionary_of_sent_scores.keys(): \n",
    "                    if any(word in k for word in keywords):\n",
    "                        keywords_sentences_list.append(k)\n",
    "    \n",
    "            \n",
    "            keywords_sentences_list = keywords_sentences_list + sorted_dict\n",
    "        \n",
    "            self.df.loc[i, 'summary'] = \" \".join(keywords_sentences_list[:summary_sent_len])\n",
    "            \n",
    "#             including tagging here\n",
    "#             self.df.loc[i,'topic_tagging'] = \n",
    "            \n",
    "        return self.df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 'this is my, . - ! ciytâ€œ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is my, . - ! ciyt\n"
     ]
    }
   ],
   "source": [
    "text = ''.join((c for c in y if ord(c) < 128))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "if __name__ == \"__main__\":\n",
    "#     loading data\n",
    "#     df = pd.read_csv('tennis_articles.csv',encoding = 'unicode_escape')\n",
    "#     creating copy of data\n",
    "    \n",
    "    df = pd.read_excel('COPS Automation Web Scrape\\Master_consolidated3.xlsx')\n",
    "    \n",
    "    df.rename(columns={'Brief Description':'article_text',\n",
    "                        'Headlines':'article_title',\n",
    "                        'Link':'source',\n",
    "                        }, inplace=True)\n",
    "    \n",
    "    df_nltk = df.copy()\n",
    "    df_spacy = df.copy()\n",
    "#     creating instances of class\n",
    "    spacy_nlp_model = text_summary(model='spacy')\n",
    "    nltk_nlp_model = text_summary(model='nltk')\n",
    "    \n",
    "#     Remove non ascii characters\n",
    "    df_spacy['article_text'] = df_spacy['article_text'].apply(spacy_nlp_model.remove_ascii)\n",
    "    df_nltk['article_text'] = df_nltk['article_text'].apply(nltk_nlp_model.remove_ascii)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#     creating sentences\n",
    "    df_spacy = spacy_nlp_model.create_sent(df_spacy)\n",
    "    df_nltk = nltk_nlp_model.create_sent(df_nltk)\n",
    "#     creating word dictionaries\n",
    "    df_spacy = spacy_nlp_model.create_words_frequency_dictionary(df_spacy)\n",
    "    df_nltk = nltk_nlp_model.create_words_frequency_dictionary(df_nltk)\n",
    "#     It is a test function\n",
    "    df_spacy = spacy_nlp_model.insert_max_word_key_value(df_spacy)\n",
    "    df_nltk = nltk_nlp_model.insert_max_word_key_value(df_nltk)\n",
    "\n",
    "    \n",
    "# this function has one bug, it is changing the main column of frequency also.\n",
    "    df_spacy['weighted_freq'] = df_spacy['frequency'].apply(spacy_nlp_model.create_word_weighted_freq)\n",
    "    df_nltk['weighted_freq'] = df_nltk['frequency'].apply(nltk_nlp_model.create_word_weighted_freq)\n",
    "# loading key words    \n",
    "    k = pd.read_excel('keyword.xlsx')\n",
    "\n",
    "    kwords = list(k['Keywords'])\n",
    "    \n",
    "#     Article tagging\n",
    "#     spacy_kwords_dict = spacy_nlp_model.create_synonms_from_keywords(kwords)\n",
    "#     nltk_kwords_dict = nltk_nlp_model.create_synonms_from_keywords(kwords)\n",
    "    \n",
    "#     df_spacy = spacy_nlp_model.insert_topics_of_article(df_spacy, spacy_kwords_dict)\n",
    "#     df_nltk = nltk_nlp_model.insert_topics_of_article(df_nltk, nltk_kwords_dict)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     for spacy first  \n",
    "    df_spacy = spacy_nlp_model.create_summary(df_spacy, keywords= kwords)\n",
    "#     for nltk \n",
    "    df_nltk = nltk_nlp_model.create_summary(df_nltk)\n",
    "#     saving the ouput in csv file\n",
    "\n",
    "\n",
    "#     removing extra columns before writing\n",
    "    df_spacy.drop(columns=['nlp_docs', 'sentence_list', 'tokenized_sents',\n",
    "       'tokenized_sents_copy', 'frequency', 'max', 'weighted_freq',\n",
    "       'sentences_scores_dict']).to_csv('spacy_summary.csv')\n",
    "    df_nltk.drop(columns=['sentence_list', 'tokenized_sents',\n",
    "       'tokenized_sents_copy', 'frequency', 'max', 'weighted_freq',\n",
    "       'sentences_scores_dict']).to_csv('nltk_summary.csv')\n",
    "    \n",
    "    # add one more thing. summary sentences should be in same orders as in \n",
    "    # articles.\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
